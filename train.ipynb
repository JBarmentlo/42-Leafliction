{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiated loader on folder /home/joep/Code/Leafliction/images. Found 7233 images.\n",
      "im.shape = torch.Size([3, 256, 256])\n",
      "y.shape = torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from leafy.loader import ImageLoader\n",
    "from leafy.trainloader import ImageDataset\n",
    "\n",
    "data_folder = Path(\"./images\")\n",
    "loader = ImageLoader(data_folder=data_folder)\n",
    "image_db = ImageDataset(loader)\n",
    "\n",
    "# class_distribution = loader.get_better_class_distribution()\n",
    "im, y = image_db[0]\n",
    "num_classes = len(y)\n",
    "print(f\"{im.shape = }\\n{y.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from icecream import ic\n",
    "from torch.nn import Module\n",
    "\n",
    "class BasicClassifier(Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BasicClassifier, self).__init__()\n",
    "        resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.fc = nn.Linear(2048, 8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.fc(x.view(x.size(0), -1))\n",
    "        return x\n",
    "        # return F.softmax(x, dim=1) # will break for unbatched data\n",
    "\n",
    "\n",
    "net = BasicClassifier(num_classes = 8)\n",
    "net = net.cuda()\n",
    "\n",
    "\n",
    "preprocess = ResNet50_Weights.DEFAULT.transforms()\n",
    "# resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8a62cf0aff4f49934763c0ed3706b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joep/Code/Leafliction/venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0994, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(1.7465, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(1.3465, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.8431, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.5081, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.4990, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.2739, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.2157, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1069, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0806, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.2069, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1101, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1561, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1209, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.3574, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0600, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0905, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.4362, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.2492, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1803, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0946, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1228, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0347, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.3547, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1048, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.2703, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1095, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.2452, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1838, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0689, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1163, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0797, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1087, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1130, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0875, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.3302, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.2092, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0652, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.2542, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0712, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.2767, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0835, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0247, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1031, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0754, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0584, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0410, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.3455, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0408, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1866, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0328, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0671, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0419, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1193, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0796, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0606, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1601, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0371, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0134, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0287, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0773, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0697, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0125, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0437, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0564, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0212, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0054, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0452, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0155, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0106, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0218, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1287, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1249, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0265, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1001, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0914, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1103, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0062, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0066, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0872, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0496, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0602, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0481, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0336, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0159, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0156, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0261, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0528, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0046, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0760, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0265, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0098, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1396, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1684, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0110, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0551, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0574, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0067, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.2371, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0770, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0720, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1798, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0317, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0557, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0062, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0484, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.1263, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0061, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0773, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0716, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0465, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.0204, device='cuda:0', grad_fn=<DivBackward1>)\n",
      "tensor(0.5662, device='cuda:0', grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "trainloader = DataLoader(image_db, 64, shuffle = True)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "optim = Adam(net.parameters())\n",
    "for x, y in tqdm(trainloader):\n",
    "    optim.zero_grad()\n",
    "    x = x.cuda()\n",
    "    x = preprocess(x)\n",
    "    y = y.cuda()\n",
    "    y_hat = net(x)\n",
    "    loss = ce_loss(y_hat, y)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3222821664b4405196c8c1b5273906c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joep/Code/Leafliction/venv/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.]]) tensor([[4.0701e-02, 2.4350e-07, 2.4290e-01, 7.7504e-08, 1.1697e-06, 1.8461e-08,\n",
      "         1.0265e-05, 7.1639e-01],\n",
      "        [8.1730e-11, 2.1488e-08, 1.3448e-08, 1.0000e+00, 4.0811e-08, 9.3865e-10,\n",
      "         3.3959e-06, 7.4941e-11],\n",
      "        [9.7715e-01, 1.1040e-08, 2.2852e-02, 6.1955e-08, 1.1246e-07, 5.1850e-08,\n",
      "         7.4757e-09, 1.8146e-07],\n",
      "        [4.2368e-06, 2.6451e-06, 2.6310e-06, 4.7705e-05, 9.4700e-06, 3.2952e-05,\n",
      "         9.9975e-01, 1.4834e-04],\n",
      "        [2.9078e-06, 2.9744e-07, 5.4163e-07, 3.2648e-06, 6.3254e-07, 3.4230e-07,\n",
      "         9.9998e-01, 9.8089e-06],\n",
      "        [9.1493e-11, 1.4113e-10, 1.0000e+00, 2.9401e-13, 3.0582e-10, 4.8019e-13,\n",
      "         3.6053e-14, 2.1792e-15],\n",
      "        [1.3501e-06, 8.2937e-05, 9.1119e-06, 9.9905e-01, 9.1452e-06, 4.1160e-06,\n",
      "         8.4674e-04, 1.4677e-06],\n",
      "        [9.9001e-01, 3.9214e-10, 9.9890e-03, 4.9006e-09, 7.3235e-09, 4.1319e-09,\n",
      "         2.7602e-09, 3.1719e-09],\n",
      "        [1.2435e-05, 9.4775e-08, 1.7776e-05, 2.9470e-07, 2.5502e-08, 9.9995e-01,\n",
      "         2.8140e-07, 2.2687e-05],\n",
      "        [8.6302e-04, 3.3207e-05, 1.0498e-03, 2.7602e-05, 9.8756e-06, 9.9776e-01,\n",
      "         2.1581e-05, 2.3208e-04],\n",
      "        [1.1391e-06, 1.3731e-09, 1.8963e-05, 2.1506e-09, 1.6959e-10, 9.9998e-01,\n",
      "         3.0636e-09, 2.5113e-06],\n",
      "        [2.4905e-05, 2.2394e-04, 4.3351e-05, 8.2076e-04, 9.9845e-01, 9.0834e-06,\n",
      "         4.1726e-04, 1.3787e-05],\n",
      "        [3.4994e-05, 3.0371e-06, 9.6676e-06, 2.9360e-04, 2.7558e-05, 1.1707e-05,\n",
      "         9.9738e-01, 2.2426e-03],\n",
      "        [5.0882e-08, 1.6543e-06, 2.3006e-07, 1.6263e-07, 1.0000e+00, 3.3739e-08,\n",
      "         3.8122e-10, 1.0767e-10],\n",
      "        [6.2489e-07, 9.9996e-01, 1.2840e-05, 7.4948e-06, 1.3521e-05, 2.2509e-06,\n",
      "         8.7270e-07, 1.9949e-08],\n",
      "        [9.6645e-03, 1.0498e-04, 1.7745e-03, 2.9496e-04, 2.9501e-05, 9.8624e-01,\n",
      "         1.5106e-04, 1.7432e-03],\n",
      "        [4.9864e-10, 1.3622e-11, 1.0000e+00, 6.8787e-12, 1.0018e-10, 7.0437e-13,\n",
      "         2.6527e-12, 1.0397e-14],\n",
      "        [9.7757e-01, 1.7692e-08, 2.2432e-02, 1.1210e-07, 1.8674e-07, 7.4500e-08,\n",
      "         2.9327e-08, 1.3581e-06],\n",
      "        [2.0396e-02, 1.8858e-03, 8.9176e-03, 1.0417e-03, 1.1709e-03, 9.6386e-01,\n",
      "         8.6387e-04, 1.8617e-03],\n",
      "        [9.4386e-01, 2.2077e-09, 5.6139e-02, 1.2047e-08, 7.8354e-09, 5.9351e-09,\n",
      "         1.1863e-08, 1.8568e-07],\n",
      "        [2.4886e-01, 6.3412e-08, 7.5114e-01, 8.7565e-08, 4.8064e-07, 1.0224e-07,\n",
      "         3.8089e-08, 2.0757e-07],\n",
      "        [5.4641e-06, 3.8445e-05, 7.7704e-05, 9.8936e-01, 1.0562e-04, 4.8638e-06,\n",
      "         1.0402e-02, 5.4806e-06],\n",
      "        [4.9946e-04, 2.2820e-08, 4.2080e-03, 5.7424e-09, 1.5303e-09, 9.9493e-01,\n",
      "         1.4096e-07, 3.6585e-04],\n",
      "        [1.2026e-07, 1.3707e-10, 8.4333e-08, 1.3682e-10, 1.2200e-11, 1.0000e+00,\n",
      "         1.6831e-10, 6.5819e-08],\n",
      "        [1.5411e-05, 7.3142e-06, 4.3091e-05, 8.4160e-04, 1.5845e-04, 5.0633e-06,\n",
      "         9.9885e-01, 7.8048e-05],\n",
      "        [4.0507e-06, 2.0619e-05, 1.5663e-04, 9.7449e-01, 4.3805e-05, 2.8441e-06,\n",
      "         2.5283e-02, 3.7838e-06],\n",
      "        [1.3025e-02, 7.4344e-04, 9.5349e-01, 1.9383e-03, 2.9740e-02, 1.9257e-04,\n",
      "         7.6185e-04, 1.0540e-04],\n",
      "        [1.2542e-08, 1.4492e-08, 2.5272e-07, 1.0848e-03, 1.0644e-06, 5.1585e-08,\n",
      "         9.9891e-01, 9.3209e-07],\n",
      "        [8.5576e-09, 1.0929e-12, 1.9498e-09, 4.3979e-13, 3.5591e-14, 1.0000e+00,\n",
      "         2.0760e-12, 7.0448e-08],\n",
      "        [9.5740e-07, 8.3700e-09, 8.6826e-07, 7.2029e-09, 3.7456e-10, 1.0000e+00,\n",
      "         1.9713e-08, 1.0528e-06],\n",
      "        [8.4262e-07, 6.5903e-07, 2.6883e-07, 1.6434e-05, 2.8252e-06, 5.1020e-07,\n",
      "         9.9997e-01, 1.1745e-05],\n",
      "        [1.3479e-05, 9.9884e-01, 2.5642e-04, 3.2346e-05, 8.4622e-04, 5.3024e-06,\n",
      "         5.4993e-06, 3.6430e-07],\n",
      "        [2.6676e-04, 1.3977e-03, 5.7900e-04, 9.9271e-01, 1.0085e-03, 9.2062e-04,\n",
      "         2.7541e-03, 3.6102e-04],\n",
      "        [7.1279e-10, 7.1400e-08, 1.0000e+00, 7.6999e-10, 1.7584e-07, 1.3005e-10,\n",
      "         7.5219e-11, 6.3285e-13],\n",
      "        [3.1107e-07, 3.6676e-07, 1.4270e-07, 8.6684e-06, 4.1478e-06, 2.2185e-07,\n",
      "         9.9998e-01, 2.1682e-06],\n",
      "        [1.2003e-06, 1.0891e-06, 4.9541e-07, 1.5258e-05, 1.2935e-05, 1.5957e-06,\n",
      "         9.9996e-01, 1.1805e-05],\n",
      "        [1.1575e-07, 8.7094e-08, 8.1080e-08, 3.3300e-06, 6.5908e-07, 5.4058e-08,\n",
      "         9.9999e-01, 3.6674e-06],\n",
      "        [5.2289e-05, 5.7787e-05, 4.7550e-05, 1.8574e-03, 2.3245e-04, 4.2796e-05,\n",
      "         9.9748e-01, 2.3291e-04],\n",
      "        [1.5977e-01, 1.3657e-07, 8.4023e-01, 2.3646e-07, 1.1585e-06, 4.2818e-07,\n",
      "         1.9596e-08, 4.9084e-08],\n",
      "        [3.4474e-02, 2.3452e-07, 1.3372e-01, 5.9650e-08, 1.2288e-05, 2.9847e-08,\n",
      "         1.2561e-05, 8.3178e-01],\n",
      "        [5.6859e-04, 8.8948e-04, 7.7152e-04, 9.8242e-01, 5.9674e-04, 3.1148e-04,\n",
      "         1.4268e-02, 1.7634e-04],\n",
      "        [1.6646e-06, 3.5535e-09, 5.0228e-07, 3.4676e-09, 1.8459e-10, 1.0000e+00,\n",
      "         4.8588e-09, 7.3478e-07],\n",
      "        [2.7440e-01, 1.1355e-06, 7.2551e-01, 1.2872e-06, 1.4066e-06, 1.4367e-06,\n",
      "         3.8982e-06, 8.1683e-05],\n",
      "        [3.9853e-04, 3.5654e-05, 2.8281e-03, 3.1602e-03, 2.2734e-04, 2.3616e-04,\n",
      "         9.9260e-01, 5.1331e-04],\n",
      "        [2.7231e-01, 2.0196e-10, 7.2769e-01, 3.2104e-10, 7.0303e-10, 2.6713e-10,\n",
      "         9.5299e-10, 2.9609e-08],\n",
      "        [2.2521e-08, 1.2570e-11, 1.8824e-08, 7.6763e-12, 1.0102e-12, 1.0000e+00,\n",
      "         2.3552e-11, 2.5213e-08],\n",
      "        [1.3921e-06, 9.3344e-06, 4.6559e-07, 1.3678e-05, 5.2161e-06, 1.4248e-06,\n",
      "         9.9996e-01, 5.6424e-06],\n",
      "        [1.3655e-11, 1.7285e-08, 1.0000e+00, 7.6148e-12, 4.9033e-08, 8.2014e-12,\n",
      "         1.8521e-13, 3.8991e-15],\n",
      "        [7.3317e-01, 6.0887e-07, 2.6681e-01, 1.2004e-06, 2.1299e-06, 1.3889e-06,\n",
      "         7.1420e-07, 1.3077e-05],\n",
      "        [3.2145e-11, 2.9649e-13, 1.0000e+00, 2.2201e-15, 1.4712e-12, 2.2916e-15,\n",
      "         1.6261e-15, 9.8519e-18],\n",
      "        [1.3535e-10, 1.0610e-09, 1.0000e+00, 2.2049e-12, 3.5051e-08, 2.4665e-12,\n",
      "         5.4611e-13, 3.6444e-15],\n",
      "        [5.3358e-05, 1.0774e-04, 3.9685e-05, 9.9966e-01, 1.8875e-05, 3.9466e-05,\n",
      "         7.5821e-05, 4.5251e-06],\n",
      "        [5.7419e-01, 9.9069e-08, 4.2575e-01, 1.5002e-07, 3.5806e-07, 1.1394e-07,\n",
      "         5.3220e-07, 6.1728e-05],\n",
      "        [1.8247e-04, 8.0669e-04, 3.0910e-04, 9.9633e-01, 1.2812e-03, 9.2016e-04,\n",
      "         1.4496e-04, 2.6331e-05],\n",
      "        [7.9887e-01, 1.0251e-06, 2.0112e-01, 2.1793e-06, 3.7477e-06, 2.9601e-06,\n",
      "         7.0681e-07, 4.2036e-06],\n",
      "        [4.3040e-08, 2.1060e-06, 2.3113e-07, 9.1875e-07, 1.0000e+00, 3.8295e-08,\n",
      "         8.9961e-09, 8.4031e-10],\n",
      "        [2.6722e-14, 3.9159e-11, 1.0000e+00, 1.2522e-14, 7.0464e-10, 1.4000e-14,\n",
      "         3.6353e-16, 2.8188e-18],\n",
      "        [7.7410e-08, 4.6347e-08, 1.6980e-07, 2.9215e-05, 5.3932e-06, 9.0430e-08,\n",
      "         9.9996e-01, 2.4809e-06],\n",
      "        [3.8731e-05, 3.9504e-04, 1.2073e-04, 2.9112e-03, 9.9624e-01, 4.0731e-05,\n",
      "         2.4476e-04, 5.7003e-06],\n",
      "        [6.0934e-07, 7.0025e-05, 1.5850e-05, 9.9876e-01, 4.4180e-05, 7.6714e-06,\n",
      "         1.1051e-03, 9.6863e-07],\n",
      "        [1.7352e-07, 1.9835e-10, 1.0338e-07, 1.2954e-10, 1.3063e-11, 1.0000e+00,\n",
      "         4.3714e-10, 3.4640e-07],\n",
      "        [4.2537e-07, 1.1378e-06, 1.3800e-07, 1.0220e-05, 2.3043e-06, 1.0812e-07,\n",
      "         9.9998e-01, 3.1537e-06],\n",
      "        [7.0753e-05, 2.5886e-08, 1.1748e-04, 1.1192e-07, 1.8487e-06, 5.0151e-08,\n",
      "         1.6950e-04, 9.9964e-01],\n",
      "        [9.4453e-08, 1.1195e-06, 5.1674e-07, 9.9997e-01, 1.2416e-06, 1.3172e-07,\n",
      "         2.5540e-05, 1.0828e-08]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19706/1189034497.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_hat = F.softmax(net(preprocess(x)))\n"
     ]
    }
   ],
   "source": [
    "for x, y in tqdm(trainloader):\n",
    "    break\n",
    "\n",
    "net = net.cpu()\n",
    "\n",
    "y_hat = F.softmax(net(preprocess(x)))\n",
    "print(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_index = torch.argmax(y, dim=1)\n",
    "y_hat_index = torch.argmax(y_hat, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "         True,  True, False,  True, False,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_index == y_hat_index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
